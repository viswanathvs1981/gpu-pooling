# Example: PyTorch training with automatic resource prediction
# TensorFusion will analyze the workload and predict GPU requirements

apiVersion: v1
kind: Pod
metadata:
  name: pytorch-training-auto
  labels:
    app: ml-training
    framework: pytorch
    task: training
  annotations:
    # Enable TensorFusion
    tensor-fusion.ai/enabled: "true"
    
    # Enable ML-based auto-resource prediction
    tensor-fusion.ai/auto-resources: "true"
    
    # Specify pool (optional)
    tensor-fusion.ai/pool-name: "default"
    
    # QoS level (optional)
    tensor-fusion.ai/qos: "medium"

spec:
  containers:
  - name: training
    image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
    
    command:
    - python
    - -c
    - |
      import torch
      import time
      print(f"CUDA available: {torch.cuda.is_available()}")
      if torch.cuda.is_available():
          print(f"GPU: {torch.cuda.get_device_name(0)}")
          print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
      
      # Simulate training
      print("Starting training simulation...")
      for epoch in range(10):
          print(f"Epoch {epoch+1}/10")
          time.sleep(30)
      
      print("Training complete!")
    
    # No resource requests needed - TensorFusion will predict and allocate
  
  restartPolicy: Never

