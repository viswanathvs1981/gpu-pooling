#!/bin/bash

set -uo pipefail

GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

success() { echo -e "${GREEN}âœ… $1${NC}"; }
info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
warn() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }

banner() {
cat <<'EOF'
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     USE CASE 7: vLLM Deployment & Inference                    â•‘
â•‘     Problem: Need high-performance LLM serving                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
}

banner
echo ""

info "ğŸ“– This demo shows:"
echo "   â€¢ What vLLM is and why it's needed"
echo "   â€¢ Key features: PagedAttention, continuous batching"
echo "   â€¢ Performance comparison vs alternatives"
echo "   â€¢ Integration with Tensor Fusion GPU sharing"
echo "   â€¢ LoRA adapter support"
echo ""
sleep 2

# Step 1: Explain vLLM
info "ğŸ¯ Step 1: Understanding vLLM"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "What is vLLM?"
echo "  vLLM (Very Large Language Model serving) is a high-performance"
echo "  inference engine optimized for serving LLMs efficiently."
echo ""
echo "Why Not Just Use PyTorch/HuggingFace?"
echo "  âŒ Naive PyTorch:"
echo "     â€¢ Processes 1 request at a time"
echo "     â€¢ Wastes 70% of GPU memory"
echo "     â€¢ 5-10 requests/second"
echo "     â€¢ Poor memory management"
echo ""
echo "  âœ… vLLM Optimizations:"
echo "     â€¢ Processes 20+ requests simultaneously"
echo "     â€¢ Uses 90% of GPU memory efficiently"
echo "     â€¢ 40-50 requests/second"
echo "     â€¢ PagedAttention for memory"
echo "     â€¢ Continuous batching"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 2: PagedAttention explained
info "ğŸ§  Step 2: PagedAttention Technology"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "The Memory Problem:"
echo "  Traditional systems allocate memory for worst case"
echo "  â€¢ Max sequence length: 4096 tokens"
echo "  â€¢ Each token needs ~500KB (KV cache)"
echo "  â€¢ Allocation: 4096 Ã— 500KB = 2GB per request"
echo "  â€¢ If only 100 tokens used â†’ 97% wasted!"
echo ""
echo "vLLM's Solution: PagedAttention"
echo "  â€¢ Allocate memory in small 'pages' (like OS virtual memory)"
echo "  â€¢ Need 100 tokens? Get 100 tokens worth - no waste"
echo "  â€¢ Dynamic allocation as conversation grows"
echo "  â€¢ Pages can be shared across requests (prefix caching)"
echo ""
echo "  Result: 3-5Ã— more requests fit in same GPU! ğŸš€"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 3: Continuous batching
info "âš¡ Step 3: Continuous Batching"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Traditional Batching:"
echo "  â€¢ Batch 8 requests together"
echo "  â€¢ All requests finish at same time"
echo "  â€¢ Short requests wait for long ones"
echo "  â€¢ GPU idle between batches"
echo ""
echo "vLLM Continuous Batching:"
echo "  â€¢ Requests join/leave batch dynamically"
echo "  â€¢ Short request done? Add new one immediately"
echo "  â€¢ No waiting, no idle time"
echo "  â€¢ GPU always working at 100%"
echo ""
echo "  Example:"
echo "    Time 0: Batch [A, B, C, D, E, F, G, H]"
echo "    Time 1: A done â†’ replace with I: [I, B, C, D, E, F, G, H]"
echo "    Time 2: C done â†’ replace with J: [I, B, J, D, E, F, G, H]"
echo "    ...continuous processing..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 4: Performance comparison
info "ğŸ“Š Step 4: Performance Comparison"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Benchmark: Llama-3-8B on NVIDIA A100 (80GB)"
echo ""
echo "Metric              â”‚ PyTorch â”‚ HF Transformers â”‚ vLLM   â”‚ Improvement"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo "Throughput (req/s)  â”‚    8    â”‚       12        â”‚   48   â”‚   6Ã—"
echo "Latency (p99)       â”‚  2.1s   â”‚      1.5s       â”‚ 0.35s  â”‚   6Ã—"
echo "GPU Memory Used     â”‚  45GB   â”‚      52GB       â”‚  72GB  â”‚  1.6Ã—"
echo "Max Concurrent      â”‚    4    â”‚        8        â”‚   32   â”‚   8Ã—"
echo "Cost per 1M tokens  â”‚  \$0.50 â”‚      \$0.35     â”‚ \$0.08 â”‚  6Ã— cheaper"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 5: Check if vLLM is deployed
info "ğŸ” Step 5: Checking vLLM Deployment Status"
echo ""

if [ -f "../scripts/deploy-vllm.sh" ]; then
  success "vLLM deployment script found: scripts/deploy-vllm.sh"
  info "  To deploy vLLM, run: bash scripts/deploy-vllm.sh"
else
  info "vLLM deployment script location: scripts/deploy-vllm.sh"
fi
echo ""

VLLM_PODS=$(kubectl get pods -n tensor-fusion-sys -l app=vllm --no-headers 2>/dev/null | wc -l | tr -d ' ')
if [ "$VLLM_PODS" -gt 0 ]; then
  success "vLLM pods found in cluster: $VLLM_PODS"
  echo ""
  kubectl get pods -n tensor-fusion-sys -l app=vllm -o wide 2>/dev/null
else
  warn "vLLM not currently deployed"
  info "  Deploy with: bash scripts/deploy-vllm.sh"
fi
echo ""
sleep 2

# Step 6: LoRA support
info "ğŸ¨ Step 6: LoRA Adapter Support"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "vLLM + LoRA = Multi-Tenant Serving"
echo ""
echo "Traditional Approach (1 model per customer):"
echo "  â€¢ Customer A: 16GB model file"
echo "  â€¢ Customer B: 16GB model file"
echo "  â€¢ Customer C: 16GB model file"
echo "  â€¢ Total: 48GB, 3 GPUs needed"
echo ""
echo "vLLM + LoRA Approach:"
echo "  â€¢ Base model: 16GB (loaded once)"
echo "  â€¢ Customer A adapter: 50MB"
echo "  â€¢ Customer B adapter: 50MB"
echo "  â€¢ Customer C adapter: 50MB"
echo "  â€¢ Total: 16.15GB, 1 GPU serves all!"
echo ""
echo "LoRA Switching:"
echo "  â€¢ Load base model once (5 seconds)"
echo "  â€¢ Switch adapters per request (50ms)"
echo "  â€¢ 100+ customers on same GPU"
echo "  â€¢ Each gets customized model"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 7: Integration with Tensor Fusion
info "ğŸ® Step 7: Integration with Tensor Fusion"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "vLLM + Tensor Fusion = Maximum Efficiency"
echo ""
echo "Layer 3 (vLLM): Efficient inference engine"
echo "     â†“"
echo "Layer 4 (Tensor Fusion): GPU virtualization"
echo "     â†“"
echo "Layer 5/6 (CUDA/Hardware): Physical GPUs"
echo ""
echo "Example Deployment:"
echo "  â€¢ 1 Physical A100 (80GB)"
echo "  â€¢ Tensor Fusion creates 3 vGPUs:"
echo "     - vGPU-1 (30GB): vLLM serving Llama-3-8B"
echo "     - vGPU-2 (30GB): vLLM serving CodeLlama-13B"
echo "     - vGPU-3 (20GB): vLLM serving Mistral-7B"
echo "  â€¢ All isolated, all efficient"
echo "  â€¢ Cost: 1 GPU serves 3 workloads"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 3

# Step 8: Deployment configuration
info "âš™ï¸  Step 8: Typical vLLM Deployment Configuration"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
cat << 'YAML'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        env:
        - name: MODEL_NAME
          value: "meta-llama/Llama-3-8b"
        - name: TENSOR_PARALLEL_SIZE
          value: "1"
        - name: MAX_MODEL_LEN
          value: "4096"
        - name: GPU_MEMORY_UTILIZATION
          value: "0.9"  # Use 90% of GPU
        resources:
          limits:
            nvidia.com/gpu: 1
        command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        - --model
        - $(MODEL_NAME)
        - --tensor-parallel-size
        - $(TENSOR_PARALLEL_SIZE)
        - --max-model-len
        - $(MAX_MODEL_LEN)
        - --gpu-memory-utilization
        - $(GPU_MEMORY_UTILIZATION)
        - --enable-lora  # LoRA support!
YAML
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 9: API usage example
info "ğŸ“¡ Step 9: Using vLLM API (OpenAI-Compatible)"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "vLLM exposes OpenAI-compatible API:"
echo ""
cat << 'CURL'
curl http://vllm-service:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3-8b",
    "prompt": "Explain quantum computing",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# With LoRA adapter:
curl http://vllm-service:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3-8b",
    "prompt": "Analyze this legal contract",
    "lora_id": "legal-contract-v2",  # Custom adapter!
    "max_tokens": 500
  }'
CURL
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Summary
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
success "ğŸ¯ Key Takeaways:"
echo "   âœ“ vLLM provides 6Ã— better throughput than naive PyTorch"
echo "   âœ“ PagedAttention optimizes memory (3-5Ã— more requests per GPU)"
echo "   âœ“ Continuous batching keeps GPU at 100% utilization"
echo "   âœ“ LoRA support enables multi-tenant serving (100+ customers/GPU)"
echo "   âœ“ OpenAI-compatible API for easy migration"
echo "   âœ“ Integrates with Tensor Fusion for maximum efficiency"
echo "   âœ“ 6Ã— cost reduction vs traditional serving"
echo ""
info "ğŸ’¡ Use Case: High-throughput LLM serving, multi-tenant platforms, cost optimization"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

info "ğŸ’¡ Next Steps:"
echo "  1. Deploy vLLM: bash scripts/deploy-vllm.sh"
echo "  2. Test inference: curl http://vllm-service:8000/v1/models"
echo "  3. Add LoRA adapters: See demo 08-lora-training.sh"
echo ""
info "Demo complete!"
sleep 2

