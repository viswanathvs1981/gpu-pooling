#!/bin/bash

set -uo pipefail

GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

success() { echo -e "${GREEN}‚úÖ $1${NC}"; }
info() { echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"; }
warn() { echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"; }

banner() {
cat <<'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     USE CASE 8: LoRA Training & Custom Model Creation         ‚ïë
‚ïë     Problem: Custom AI models are expensive & slow to train    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
}

banner
echo ""

info "üìñ This demo shows:"
echo "   ‚Ä¢ What LoRA is and why it matters"
echo "   ‚Ä¢ Cost comparison: LoRA vs full fine-tuning"
echo "   ‚Ä¢ Training workflow from data to deployment"
echo "   ‚Ä¢ Multi-tenant model serving"
echo "   ‚Ä¢ Real-world use cases"
echo ""
sleep 2

# Step 1: The Problem
info "‚ùå Step 1: The Traditional Fine-Tuning Problem"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Scenario: You need a legal contract analysis AI"
echo ""
echo "Traditional Full Fine-Tuning:"
echo "  ‚Ä¢ Train ALL 8 billion parameters"
echo "  ‚Ä¢ Hardware: 64√ó A100 GPUs"
echo "  ‚Ä¢ Time: 48+ hours"
echo "  ‚Ä¢ Cost: ~\$5,000"
echo "  ‚Ä¢ Storage: 16GB model file per customer"
echo "  ‚Ä¢ Memory: Can't serve multiple on same GPU"
echo ""
echo "For 100 Customers:"
echo "  ‚Ä¢ Training cost: 100 √ó \$5,000 = \$500,000 üí∏"
echo "  ‚Ä¢ Storage: 100 √ó 16GB = 1.6TB"
echo "  ‚Ä¢ GPUs needed: 100 (one per customer)"
echo ""
echo "This doesn't scale! üò±"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 2: LoRA Solution
info "‚úÖ Step 2: The LoRA Solution"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "LoRA: Low-Rank Adaptation of Large Language Models"
echo ""
echo "The Key Insight:"
echo "  ‚Ä¢ Base model (Llama-3-8B) already knows language"
echo "  ‚Ä¢ Don't retrain everything, just add small 'adapters'"
echo "  ‚Ä¢ Adapters are 0.1% the size of the full model"
echo "  ‚Ä¢ Base model + adapter = customized model"
echo ""
echo "How It Works:"
echo "  1. Freeze base model (8B params) ‚ùÑÔ∏è"
echo "  2. Add small adapter layers (8M params) ‚ûï"
echo "  3. Train ONLY the adapters"
echo "  4. At inference: base + adapter"
echo ""
echo "Think of it like:"
echo "  ‚Ä¢ Base model = Universal translator"
echo "  ‚Ä¢ Adapter = Accent/dialect module"
echo "  ‚Ä¢ Same core, different specializations"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 3: Cost Comparison
info "üí∞ Step 3: Cost & Time Comparison"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Metric                  ‚îÇ Full Fine-Tuning ‚îÇ LoRA           ‚îÇ Savings"
echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
echo "Parameters Trained      ‚îÇ 8,000,000,000    ‚îÇ 8,000,000      ‚îÇ 1000√ó"
echo "GPUs Needed             ‚îÇ 64               ‚îÇ 1              ‚îÇ 64√ó"
echo "Training Time           ‚îÇ 48 hours         ‚îÇ 2 hours        ‚îÇ 24√ó"
echo "Training Cost           ‚îÇ \$5,000          ‚îÇ \$100          ‚îÇ 50√ó"
echo "Model File Size         ‚îÇ 16GB             ‚îÇ 50MB           ‚îÇ 320√ó"
echo "Memory Per Model        ‚îÇ 16GB             ‚îÇ 50MB (shared)  ‚îÇ 320√ó"
echo "Deploy Time             ‚îÇ 5 minutes        ‚îÇ 50ms           ‚îÇ 6000√ó"
echo "Models Per GPU          ‚îÇ 1                ‚îÇ 100+           ‚îÇ 100√ó"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 4: LoRA Architecture
info "üèóÔ∏è  Step 4: LoRA Architecture Explained"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Original Transformer Layer:"
echo "  Input ‚Üí [Weight Matrix W (8B√ó8B)] ‚Üí Output"
echo ""
echo "LoRA-Modified Layer:"
echo "  Input ‚Üí [W (frozen)] + [A (8B√ó32) √ó B (32√ó8B)] ‚Üí Output"
echo "           ‚Üë                ‚Üë          ‚Üë"
echo "        Original        Low-rank    Low-rank"
echo "      (not trained)    (trainable) (trainable)"
echo ""
echo "Key Parameters:"
echo "  ‚Ä¢ Rank (r): Size of low-rank decomposition"
echo "    - r=8: Fast, lower quality (good for simple tasks)"
echo "    - r=32: Balanced (most common)"
echo "    - r=64: High quality (complex domains)"
echo ""
echo "  ‚Ä¢ LoRA Alpha (Œ±): Scaling factor"
echo "    - Typically Œ± = 2√ór"
echo "    - Controls how much adapter affects output"
echo ""
echo "Example Configuration:"
echo "  ‚Ä¢ Base: Llama-3-8B"
echo "  ‚Ä¢ Rank: 32"
echo "  ‚Ä¢ Alpha: 64"
echo "  ‚Ä¢ Target modules: q_proj, v_proj (attention)"
echo "  ‚Ä¢ Trainable params: ~8M (0.1% of base)"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 5: Training Workflow
info "üéì Step 5: Complete Training Workflow"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Step 1: Prepare Training Data"
echo "  ‚Ä¢ Collect domain-specific data (legal contracts, medical notes, etc.)"
echo "  ‚Ä¢ Format: Instruction-response pairs"
echo "  ‚Ä¢ Size: 1K-50K examples (much smaller than full fine-tuning!)"
echo "  ‚Ä¢ Quality > Quantity for LoRA"
echo ""
echo "Step 2: Configure Training"
cat << 'PYTHON'
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=32,                    # Rank
    lora_alpha=64,           # Scaling
    target_modules=["q_proj", "v_proj"],  # Which layers
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
PYTHON
echo ""
echo "Step 3: Training Loop"
echo "  ‚Ä¢ Batch size: 8 (fits in 1 GPU)"
echo "  ‚Ä¢ Learning rate: 3e-4"
echo "  ‚Ä¢ Epochs: 3"
echo "  ‚Ä¢ Gradient accumulation: 4 steps"
echo "  ‚Ä¢ Mixed precision: fp16 (faster)"
echo "  ‚Ä¢ Time: ~2 hours on A100"
echo ""
echo "Step 4: Save Adapter"
echo "  ‚Ä¢ Only adapter weights saved (50MB)"
echo "  ‚Ä¢ Upload to model registry"
echo "  ‚Ä¢ Ready for deployment"
echo ""
echo "Step 5: Deploy to vLLM"
echo "  ‚Ä¢ vLLM loads base model once"
echo "  ‚Ä¢ Adapter loaded per request (50ms)"
echo "  ‚Ä¢ Instant model switching"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 6: Multi-Tenant Serving
info "üè¢ Step 6: Multi-Tenant Serving Example"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Scenario: AI Platform with 100 customers"
echo ""
echo "Without LoRA:"
echo "  ‚Ä¢ 100 separate models (16GB each)"
echo "  ‚Ä¢ Need 100 GPUs (1 per customer)"
echo "  ‚Ä¢ Cost: 100 √ó \$3/hour = \$300/hour = \$219,000/month"
echo ""
echo "With LoRA + vLLM:"
echo "  ‚Ä¢ 1 base model (16GB)"
echo "  ‚Ä¢ 100 adapters (50MB each = 5GB total)"
echo "  ‚Ä¢ Total: 21GB (fits on 1 GPU!)"
echo "  ‚Ä¢ Cost: \$3/hour = \$2,190/month"
echo "  ‚Ä¢ Savings: \$216,810/month (99% reduction!) üéâ"
echo ""
echo "Request Flow:"
echo "  Customer A request ‚Üí Load adapter A ‚Üí Generate ‚Üí Response"
echo "  Customer B request ‚Üí Load adapter B ‚Üí Generate ‚Üí Response"
echo "  (Base model stays loaded, adapters swap)"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 7: Real-world Use Cases
info "üåç Step 7: Real-World Use Cases"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "Use Case 1: Legal Tech Company"
echo "  ‚Ä¢ Base: Llama-3-70B"
echo "  ‚Ä¢ Training data: 10K legal contracts"
echo "  ‚Ä¢ Adapters: Contract analysis, clause extraction, risk scoring"
echo "  ‚Ä¢ Cost: \$300 training, \$1,500/month serving"
echo "  ‚Ä¢ vs GPT-4: \$0.03/page √ó 1M pages = \$30,000/month"
echo "  ‚Ä¢ ROI: Break even in 2 weeks"
echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
echo ""
echo "Use Case 2: Healthcare AI Platform"
echo "  ‚Ä¢ Base: Med-PaLM-2"
echo "  ‚Ä¢ Training data: 50K medical notes (HIPAA-compliant)"
echo "  ‚Ä¢ Adapters: Diagnosis, treatment plans, coding"
echo "  ‚Ä¢ Benefit: Data never leaves private cloud"
echo "  ‚Ä¢ Training: \$500 one-time"
echo "  ‚Ä¢ Serving: \$2,000/month"
echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
echo ""
echo "Use Case 3: Customer Support SaaS"
echo "  ‚Ä¢ Base: Mistral-7B"
echo "  ‚Ä¢ Per-customer adapters: Company knowledge, tone, policies"
echo "  ‚Ä¢ 500 customers √ó \$50 training = \$25,000 one-time"
echo "  ‚Ä¢ vs 500 full models: \$2.5M + ongoing costs"
echo "  ‚Ä¢ Each customer gets personalized AI"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 3

# Step 8: Training Script Example
info "üìù Step 8: Example Training Script"
echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
cat << 'PYTHON'
# train_lora.py
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# 1. Load base model
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8b",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8b")

# 2. Add LoRA adapters
lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1
)
model = get_peft_model(model, lora_config)

# 3. Load training data
dataset = load_dataset("your-custom-data")

# 4. Train
trainer = Trainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        per_device_train_batch_size=8,
        learning_rate=3e-4,
        num_train_epochs=3,
        fp16=True,
        output_dir="./lora-adapter"
    )
)
trainer.train()

# 5. Save adapter (only 50MB!)
model.save_pretrained("./lora-adapter")
PYTHON
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
sleep 2

# Summary
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
success "üéØ Key Takeaways:"
echo "   ‚úì LoRA enables 50√ó cheaper customization (\$100 vs \$5,000)"
echo "   ‚úì 24√ó faster training (2 hours vs 48 hours)"
echo "   ‚úì 320√ó smaller model files (50MB vs 16GB)"
echo "   ‚úì 100+ customers can share 1 GPU"
echo "   ‚úì Perfect for multi-tenant AI platforms"
echo "   ‚úì Data stays private (train on your own infrastructure)"
echo "   ‚úì Rapid experimentation (try 10 variants for \$1,000)"
echo ""
info "üí° Use Case: SaaS platforms, enterprise AI, custom domain models"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""

info "üí° Next Steps:"
echo "  1. Prepare training data in instruction format"
echo "  2. Train adapter: python train_lora.py"
echo "  3. Deploy to vLLM: See demo 07-vllm-deployment.sh"
echo "  4. Test: curl http://vllm-service:8000/v1/completions -d '{\"lora_id\": \"my-adapter\"}'"
echo ""
info "Demo complete!"
sleep 2

