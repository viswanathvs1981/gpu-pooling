#!/bin/bash

set -uo pipefail

GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

success() { echo -e "${GREEN}âœ… $1${NC}"; }
info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
warn() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
error() { echo -e "${RED}âŒ $1${NC}"; }

banner() {
cat <<'EOF'
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     USE CASE 9: GPU Resource Monitoring & Observability       â•‘
â•‘     Problem: Need visibility into GPU usage & performance      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
}

banner
echo ""

info "ğŸ“– This demo shows:"
echo "   â€¢ Real-time GPU resource tracking"
echo "   â€¢ Multi-tenant usage breakdown"
echo "   â€¢ Cost allocation per customer"
echo "   â€¢ Performance metrics & bottlenecks"
echo "   â€¢ Alerts & anomaly detection"
echo ""
sleep 2

# Step 1: Check cluster GPU resources
info "ğŸ” Step 1: Cluster GPU Inventory"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
GPU_NODES=$(kubectl get nodes -l nvidia.com/gpu.present=true --no-headers 2>/dev/null | wc -l | tr -d ' ')

if [ "$GPU_NODES" -gt 0 ]; then
  success "GPU nodes found: $GPU_NODES"
  echo ""
  info "Node Details:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  kubectl get nodes -l nvidia.com/gpu.present=true -o custom-columns=\
NAME:.metadata.name,\
GPU-TYPE:.metadata.labels.'nvidia\.com/gpu\.product',\
GPU-COUNT:.status.capacity.'nvidia\.com/gpu',\
STATUS:.status.conditions[-1].type 2>/dev/null || echo "  Details unavailable"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
else
  warn "No GPU nodes found in cluster"
  info "  This is OK - demo will show expected monitoring output"
fi
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 2: GPU Custom Resources
info "ğŸ“Š Step 2: Tensor Fusion GPU Resources"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

GPUNODE_COUNT=$(kubectl get gpunode --all-namespaces --no-headers 2>/dev/null | wc -l | tr -d ' ')
GPU_COUNT=$(kubectl get gpu --all-namespaces --no-headers 2>/dev/null | wc -l | tr -d ' ')

if [ "$GPUNODE_COUNT" -gt 0 ]; then
  success "GPUNode resources: $GPUNODE_COUNT"
  echo ""
  info "GPUNode Status:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  kubectl get gpunode -o custom-columns=\
NAME:.metadata.name,\
TFLOPS:.status.capacity.tflops,\
VRAM:.status.capacity.vram,\
GPU-COUNT:.status.capacity.gpu 2>/dev/null
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  echo ""
fi

if [ "$GPU_COUNT" -gt 0 ]; then
  success "GPU resources: $GPU_COUNT"
  echo ""
  info "Individual GPUs:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  kubectl get gpu -o custom-columns=\
NAME:.metadata.name,\
MODEL:.spec.model,\
TFLOPS:.spec.tflops,\
VRAM:.spec.vram,\
STATUS:.status.phase 2>/dev/null | head -10
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  echo ""
fi

if [ "$GPUNODE_COUNT" -eq 0 ] && [ "$GPU_COUNT" -eq 0 ]; then
  warn "No GPU custom resources found yet"
  info "  Node discovery may still be running or no GPUs available"
fi
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 3: Resource Utilization
info "âš¡ Step 3: Real-Time GPU Utilization"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
if [ "$GPU_NODES" -gt 0 ]; then
  info "Running nvidia-smi on GPU nodes..."
  echo ""
  
  # Try to run nvidia-smi on a GPU node
  GPU_NODE=$(kubectl get nodes -l nvidia.com/gpu.present=true -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
  
  if [ -n "$GPU_NODE" ]; then
    info "Node: $GPU_NODE"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    
    # Try to get GPU metrics via a debug pod
    kubectl run nvidia-smi-test --rm -i --restart=Never \
      --image=nvidia/cuda:12.2.0-base-ubuntu22.04 \
      --overrides='{"spec":{"nodeSelector":{"nvidia.com/gpu.present":"true"}}}' \
      -- nvidia-smi --query-gpu=gpu_name,memory.total,memory.used,utilization.gpu --format=csv 2>/dev/null || \
    echo "  GPU metrics collection requires privileged access"
    
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  fi
else
  info "Example GPU Utilization (when GPUs available):"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  cat << 'TABLE'
GPU  â”‚ Name            â”‚ Memory Used â”‚ Memory Total â”‚ GPU Util â”‚ Temp
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
  0  â”‚ Tesla T4        â”‚   8.2 GB    â”‚   16 GB      â”‚   65%    â”‚ 54Â°C
  1  â”‚ Tesla T4        â”‚  12.5 GB    â”‚   16 GB      â”‚   89%    â”‚ 61Â°C
  2  â”‚ Tesla T4        â”‚   4.1 GB    â”‚   16 GB      â”‚   32%    â”‚ 48Â°C
â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€
TABLE
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
fi
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 4: Pool-Based Monitoring
info "ğŸŠ Step 4: GPU Pool Utilization"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
POOL_COUNT=$(kubectl get gpupool --all-namespaces --no-headers 2>/dev/null | wc -l | tr -d ' ')

if [ "$POOL_COUNT" -gt 0 ]; then
  success "GPU pools found: $POOL_COUNT"
  echo ""
  info "Pool Status:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  kubectl get gpupool --all-namespaces -o custom-columns=\
NAMESPACE:.metadata.namespace,\
NAME:.metadata.name,\
CAPACITY:.status.capacity.gpu,\
AVAILABLE:.status.available.gpu 2>/dev/null || echo "  Pool details unavailable"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
else
  info "Example GPU Pool Monitoring:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  cat << 'TABLE'
Pool           â”‚ Total GPUs â”‚ Allocated â”‚ Available â”‚ Util %
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
default-pool   â”‚     10     â”‚    7.5    â”‚    2.5    â”‚  75%
spot-pool      â”‚      8     â”‚    6.2    â”‚    1.8    â”‚  78%
training-pool  â”‚      4     â”‚    3.8    â”‚    0.2    â”‚  95%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€
TABLE
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
fi
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 5: Multi-Tenant Breakdown
info "ğŸ¢ Step 5: Multi-Tenant Resource Breakdown"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
QUOTA_COUNT=$(kubectl get gpuresourcequota --all-namespaces --no-headers 2>/dev/null | wc -l | tr -d ' ')

if [ "$QUOTA_COUNT" -gt 0 ]; then
  success "GPU resource quotas: $QUOTA_COUNT"
  echo ""
  info "Quota Status:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  kubectl get gpuresourcequota --all-namespaces -o custom-columns=\
NAMESPACE:.metadata.namespace,\
NAME:.metadata.name,\
GPU-QUOTA:.spec.hard.gpu,\
GPU-USED:.status.used.gpu 2>/dev/null || echo "  Quota details unavailable"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
else
  info "Example Multi-Tenant Usage:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  cat << 'TABLE'
Tenant         â”‚ Quota â”‚ Used  â”‚ Available â”‚ Usage % â”‚ Cost/Month
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
acme-corp      â”‚  3.0  â”‚  2.8  â”‚    0.2    â”‚   93%   â”‚  \$6,720
techstart-inc  â”‚  2.0  â”‚  1.5  â”‚    0.5    â”‚   75%   â”‚  \$3,600
legal-ai-co    â”‚  1.5  â”‚  1.2  â”‚    0.3    â”‚   80%   â”‚  \$2,880
med-platform   â”‚  2.5  â”‚  0.8  â”‚    1.7    â”‚   32%   â”‚  \$1,920
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 9.0 GPU quota, 6.3 GPUs used (70% utilization)
TABLE
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
fi
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 6: Cost Allocation
info "ğŸ’° Step 6: Cost Allocation & Billing"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Cost Calculation (per tenant):"
echo ""
echo "  Base Rate: \$2.40/hour per full GPU (Tesla T4)"
echo "  Billing: Per-second usage, aggregated monthly"
echo ""
echo "Example Calculation for acme-corp:"
echo "  â€¢ Usage: 2.8 vGPUs Ã— 720 hours = 2,016 GPU-hours"
echo "  â€¢ Cost: 2,016 Ã— \$2.40 = \$4,838.40"
echo "  â€¢ With Tensor Fusion sharing: \$4,838 vs \$8,640 (full GPUs)"
echo "  â€¢ Savings: \$3,802 (44%)"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
info "Monthly Cost Breakdown by Tenant:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
cat << 'TABLE'
Tenant         â”‚ vGPU Used â”‚ GPU-Hours â”‚ Cost      â”‚ vs Full GPU â”‚ Savings
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
acme-corp      â”‚    2.8    â”‚   2,016   â”‚  \$4,838  â”‚   \$8,640   â”‚  44%
techstart-inc  â”‚    1.5    â”‚   1,080   â”‚  \$2,592  â”‚   \$5,184   â”‚  50%
legal-ai-co    â”‚    1.2    â”‚     864   â”‚  \$2,074  â”‚   \$4,320   â”‚  52%
med-platform   â”‚    0.8    â”‚     576   â”‚  \$1,382  â”‚   \$3,456   â”‚  60%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                                     \$10,886     \$21,600     50% avg
TABLE
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 7: Performance Metrics
info "ğŸ“ˆ Step 7: Performance Metrics & KPIs"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Key Performance Indicators:"
echo ""
echo "  1. GPU Utilization"
echo "     â€¢ Target: > 80%"
echo "     â€¢ Current: 70%"
echo "     â€¢ Status: âš ï¸  Room for optimization"
echo ""
echo "  2. Memory Efficiency"
echo "     â€¢ VRAM allocated: 180GB / 240GB total"
echo "     â€¢ Utilization: 75%"
echo "     â€¢ Status: âœ… Good"
echo ""
echo "  3. Request Throughput"
echo "     â€¢ Inference requests: 45,230/hour"
echo "     â€¢ Average latency: 320ms"
echo "     â€¢ P99 latency: 580ms"
echo "     â€¢ Status: âœ… Meeting SLAs"
echo ""
echo "  4. Cost Efficiency"
echo "     â€¢ Cost per 1M tokens: \$0.12"
echo "     â€¢ vs Azure OpenAI: \$30/1M tokens"
echo "     â€¢ Savings: 99.6%"
echo "     â€¢ Status: âœ… Excellent"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 8: Monitoring Stack
info "ğŸ”§ Step 8: Monitoring & Observability Stack"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
PROMETHEUS_PODS=$(kubectl get pods -n prometheus --no-headers 2>/dev/null | wc -l | tr -d ' ')
GRAFANA_PODS=$(kubectl get pods -n grafana --no-headers 2>/dev/null | wc -l | tr -d ' ')
GREPTIMEDB_PODS=$(kubectl get pods -n greptimedb --no-headers 2>/dev/null | wc -l | tr -d ' ')

info "Deployed Components:"
echo ""

if [ "$PROMETHEUS_PODS" -gt 0 ]; then
  success "Prometheus: Running ($PROMETHEUS_PODS pods)"
  echo "  â€¢ Metrics collection: GPU, CPU, memory, network"
  echo "  â€¢ Scrape interval: 15s"
else
  info "Prometheus: Not deployed"
fi

if [ "$GRAFANA_PODS" -gt 0 ]; then
  success "Grafana: Running ($GRAFANA_PODS pods)"
  echo "  â€¢ Dashboards: GPU utilization, costs, performance"
  echo "  â€¢ Alerts: Anomaly detection, quota exceeded"
else
  info "Grafana: Not deployed"
fi

if [ "$GREPTIMEDB_PODS" -gt 0 ]; then
  success "GreptimeDB: Running ($GREPTIMEDB_PODS pods)"
  echo "  â€¢ Time-series storage for long-term metrics"
  echo "  â€¢ Retention: 90 days"
else
  info "GreptimeDB: Not deployed"
fi

echo ""
info "Metrics Collected:"
echo "  â€¢ GPU: Utilization, memory, temperature, power"
echo "  â€¢ Workload: Request count, latency, throughput"
echo "  â€¢ Cost: Per-tenant usage, billing aggregation"
echo "  â€¢ Performance: Model inference time, queue depth"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Step 9: Alerts & Anomaly Detection
info "ğŸš¨ Step 9: Alerts & Anomaly Detection"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Active Alert Rules:"
echo ""
echo "  1. High GPU Utilization"
echo "     â€¢ Trigger: > 95% for 5 minutes"
echo "     â€¢ Action: Scale up, alert ops team"
echo "     â€¢ Status: âœ… Normal (70%)"
echo ""
echo "  2. Quota Exceeded"
echo "     â€¢ Trigger: Tenant uses > 90% of quota"
echo "     â€¢ Action: Notify tenant, block new workloads"
echo "     â€¢ Status: âš ï¸  acme-corp at 93%"
echo ""
echo "  3. High Latency"
echo "     â€¢ Trigger: P99 > 1000ms for 10 minutes"
echo "     â€¢ Action: Check for bottlenecks, scale"
echo "     â€¢ Status: âœ… Normal (580ms)"
echo ""
echo "  4. GPU Failure"
echo "     â€¢ Trigger: GPU offline or unhealthy"
echo "     â€¢ Action: Drain workloads, page oncall"
echo "     â€¢ Status: âœ… All healthy"
echo ""
echo "  5. Cost Anomaly"
echo "     â€¢ Trigger: 50% spike in usage cost"
echo "     â€¢ Action: Investigate, notify customer"
echo "     â€¢ Status: âœ… Normal"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
sleep 2

# Summary
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
success "ğŸ¯ Key Takeaways:"
echo "   âœ“ Real-time GPU utilization tracking across all nodes"
echo "   âœ“ Multi-tenant resource breakdown & quota enforcement"
echo "   âœ“ Per-tenant cost allocation (50% savings with vGPU sharing)"
echo "   âœ“ Performance metrics: latency, throughput, efficiency"
echo "   âœ“ Proactive alerts for capacity, quotas, anomalies"
echo "   âœ“ Complete observability: Prometheus + Grafana + GreptimeDB"
echo ""
info "ğŸ’¡ Use Case: Platform operations, cost tracking, capacity planning"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

info "ğŸ’¡ Next Steps:"
echo "  1. Access Grafana: kubectl port-forward -n grafana svc/grafana 3000:80"
echo "  2. View metrics: http://localhost:3000"
echo "  3. Check quotas: kubectl get gpuresourcequota --all-namespaces"
echo "  4. Review GPU resources: kubectl get gpu,gpunode"
echo ""
info "Demo complete!"
sleep 2

